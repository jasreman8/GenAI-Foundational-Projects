{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasreman8/GenAI-Foundational-Projects/blob/main/Llama3_Medical_Text_Processing_Notebook_Ollama_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Objectives\n",
        "\n",
        "1. Access Llama3 APIs hosted with Ollama.\n",
        "2. Understand the API structure of prompts presented to Ollama servers.\n"
      ],
      "metadata": {
        "id": "PHWZiGA6EfGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "9Qd3zRsWExvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2JV-C6dYMVL",
        "outputId": "44dbd9d8-b606-4ccb-c528-62ca7c484458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-xterm in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.12/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.12/dist-packages (from colab-xterm) (6.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-xterm # https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the following steps to execute in xterm:\n",
        "\n",
        "Step 1: curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "Step 2: ollama serve"
      ],
      "metadata": {
        "id": "-S0PhJw9a2Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "K4ojy4kkZTnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue with the rest of the notebook after you run `ollama serve` in the xterm."
      ],
      "metadata": {
        "id": "Fj2O0wOlbZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q ollama"
      ],
      "metadata": {
        "id": "nlxF0mXBYWP1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama"
      ],
      "metadata": {
        "id": "2xgkTtHGYoUm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama.create(model='llama3.1', from_='llama3.1', parameters={'temperature': 0.2, 'num_ctx': 4096})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7efniU3ZAjj",
        "outputId": "4c73f57e-f1fa-47fb-f996-78812da5688b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProgressResponse(status='success', completed=None, total=None, digest=None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell will take some time to run till you see a success status because it is pulling in the model from its repo. Check the status in xterm."
      ],
      "metadata": {
        "id": "VRGocHa0byQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us access Llama3.1 hosted using the Ollama server."
      ],
      "metadata": {
        "id": "AGchXdnVGcyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You are an assistant to a hospital administration team working on extracting important information from medical notes made by doctors.\n",
        "Medical notes will be presented to you in the user input.\n",
        "Extract relevant information as mentioned below in a json format with the following schema.\n",
        "- age: integer, age of the patient\n",
        "- gender: string, can be one of male, female or other\n",
        "- diagnosis: string, can be one of migraine, diabetes, arthritis and acne\n",
        "- weight: integer, weight of the patient\n",
        "- smoking: string, can be one of yes or no\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M7EzIwZKZCgg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"\"\"\n",
        "Medical Notes:\n",
        "---\n",
        "A 35-year-old male patient, Mr. Nags, presented with symptoms\n",
        "of increased thirst, frequent urination, fatigue, and unexplained\n",
        "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
        "confirmed by elevated blood sugar levels. Mr. Nags' weight\n",
        "is 80 kgs. He has been prescribed Metformin to be taken twice daily\n",
        "with meals. It was noted during the consultation that the patient is\n",
        "a current smoker.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ClAYgxzKGilv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ollama.chat(\n",
        "    model='llama3.1',\n",
        "    messages=[\n",
        "        {'role':'system', 'content': system_message},\n",
        "        {'role':'user', 'content': user_input}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1DWJBqeKGm6i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz98B4teGzvX",
        "outputId": "e81a5ee1-dadb-45cc-f431-9a5ffd8c097a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the extracted information in JSON format:\n",
            "\n",
            "```\n",
            "{\n",
            "    \"age\": 35,\n",
            "    \"gender\": \"male\",\n",
            "    \"diagnosis\": \"diabetes\",\n",
            "    \"weight\": 80,\n",
            "    \"smoking\": \"yes\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCyPUlLFNEHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}